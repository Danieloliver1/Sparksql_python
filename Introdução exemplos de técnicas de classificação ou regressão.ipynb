{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1ySDWmgDwcZmgQyjGrUhIPXtPdQ6YAEXe","authorship_tag":"ABX9TyONbvSMiyWykDOZwKwe9c1o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Aqui estão alguns exemplos de situações em que você aplicaria técnicas de classificação ou regressão:\n","\n","**Exemplos de Aplicação de Classificação:**\n","\n","1. **Detecção de Spam:** Classificar e-mails como spam ou não spam com base no conteúdo e nas características dos e-mails.\n","\n","2. **Diagnóstico Médico:** Classificar pacientes como tendo uma determinada condição médica (por exemplo, diabetes, câncer) com base em dados clínicos.\n","\n","3. **Detecção de Fraudes:** Identificar transações fraudulentas em cartões de crédito com base em padrões de gastos e histórico de transações.\n","\n","4. **Classificação de Imagens:** Identificar objetos em imagens, como classificar imagens de animais em categorias específicas.\n","\n","5. **Análise de Sentimento:** Classificar textos, como avaliações de produtos ou postagens em redes sociais, em positivas, negativas ou neutras.\n","\n","6. **Previsão de Churn:** Prever se um cliente vai cancelar um serviço com base em comportamentos passados e características.\n","\n","**Exemplos de Aplicação de Regressão:**\n","\n","1. **Previsão de Vendas:** Prever as vendas futuras de um produto com base em dados históricos de vendas e variáveis como preço, promoções, etc.\n","\n","2. **Previsão de Preços Imobiliários:** Estimar o valor de uma propriedade com base em características como área, localização, número de quartos, etc.\n","\n","3. **Previsão de Demanda:** Estimar a quantidade de um produto que será necessária em um determinado período de tempo.\n","\n","4. **Análise de Crescimento:** Modelar o crescimento populacional ao longo dos anos usando dados históricos.\n","\n","5. **Análise de Desempenho de Investimentos:** Prever o retorno esperado de um investimento com base em dados econômicos e financeiros.\n","\n","6. **Previsão de Consumo de Energia:** Estimar o consumo de energia em um local com base em fatores como temperatura, temporada, etc.\n","\n","Em resumo, você deve aplicar técnicas de classificação quando estiver lidando com problemas de categorização, onde a saída desejada é uma classe ou categoria específica. Por outro lado, as técnicas de regressão são apropriadas quando você está prevendo valores numéricos contínuos ou modelando relações entre variáveis. A escolha entre classificação e regressão depende da natureza dos dados e dos objetivos do seu projeto."],"metadata":{"id":"qtzJsemQYhW8"}},{"cell_type":"markdown","source":["Certamente! Saber escolher entre diferentes tipos de regressão e classificação depende da natureza dos seus dados e do problema que você está tentando resolver. Vou fornecer uma breve visão geral dos tipos mais comuns de regressão e classificação, bem como algumas diretrizes para saber quando aplicar cada um deles.\n","\n","**Tipos de Regressão:**\n","\n","1. **Regressão Linear:** Usada quando você está tentando prever um valor numérico contínuo com base em variáveis independentes. A relação entre as variáveis é modelada por uma linha reta.\n","\n","2. **Regressão Logística:** Usada para classificação binária, onde a variável dependente é categórica (por exemplo, sim/não) e a saída é mapeada para uma probabilidade usando a função logística.\n","\n","3. **Regressão Polinomial:** Uma extensão da regressão linear onde você modela a relação entre as variáveis usando termos polinomiais, o que pode capturar relações mais complexas.\n","\n","4. **Regressão Ridge e Lasso:** Variantes da regressão linear que ajudam a lidar com multicolinearidade e overfitting, controlando a magnitude dos coeficientes.\n","\n","5. **Regressão Não Linear:** Usada quando a relação entre as variáveis não pode ser bem modelada por uma linha reta, envolvendo funções não lineares.\n","\n","**Tipos de Classificação:**\n","\n","1. **Regressão Logística:** Usada para problemas de classificação binária, como identificar se um e-mail é spam ou não spam.\n","\n","2. **Classificação Multiclasse:** Utilizada quando há mais de duas classes possíveis, como classificar imagens em gatos, cachorros ou pássaros.\n","\n","3. **SVM (Máquinas de Vetores de Suporte):** Pode ser usada tanto para problemas de classificação binária quanto para problemas de classificação multiclasse. Ela encontra um hiperplano que melhor separa as classes no espaço das características.\n","\n","4. **Árvores de Decisão:** Divide os dados em subconjuntos com base nas características, resultando em uma estrutura de árvore que permite a classificação.\n","\n","5. **Random Forest e Gradient Boosting:** Técnicas que combinam várias árvores de decisão para melhorar a precisão da classificação.\n","\n","**Como Aplicar:**\n","\n","1. **Entenda o Problema:** Compreenda os objetivos, os dados disponíveis, a natureza das variáveis e a tarefa que deseja realizar: regressão ou classificação.\n","\n","2. **Analise os Dados:** Realize uma análise exploratória dos dados para identificar padrões, outliers e entender as características relevantes.\n","\n","3. **Escolha do Algoritmo:** Baseie a escolha do algoritmo na natureza do problema e dos dados. Perguntas importantes incluem: a saída é numérica ou categórica? Você precisa lidar com multicolinearidade ou overfitting?\n","\n","4. **Treinamento e Avaliação:** Separe seus dados em conjuntos de treinamento e teste. Treine o modelo com os dados de treinamento e avalie seu desempenho nos dados de teste. Use métricas relevantes para avaliar a qualidade do modelo (por exemplo, precisão, recall, MSE, etc.).\n","\n","5. **Ajuste e Validação:** Dependendo do desempenho do modelo, você pode ajustar hiperparâmetros, experimentar com outras técnicas ou realizar validação cruzada para garantir a robustez do seu modelo.\n","\n","6. **Interpretação:** Ao final, interprete os resultados do seu modelo para tomar decisões ou extrair insights.\n","\n","Lembre-se de que não há uma abordagem única para todos os cenários, e a prática constante e a experimentação ajudarão a desenvolver um bom entendimento sobre quando aplicar cada técnica."],"metadata":{"id":"zKhC7lzKYqY0"}},{"cell_type":"markdown","source":["A regressão logística é frequentemente utilizada em problemas de classificação binária, mas não é estritamente uma técnica de classificação. Na verdade, é um algoritmo de aprendizado de máquina que é usado tanto para classificação quanto para modelagem de probabilidade.\n","\n","Em um problema de classificação binária, você tem duas classes possíveis (por exemplo, sim/não, positivo/negativo) e deseja prever a qual classe uma determinada entrada pertence. A regressão logística pode ser usada para calcular a probabilidade de que uma entrada pertença a uma das duas classes. Se essa probabilidade for maior que um certo limiar (geralmente 0,5), a entrada é classificada como pertencente à classe correspondente; caso contrário, é classificada na outra classe.\n","\n","A regressão logística utiliza uma função logística (ou sigmoide) para mapear a saída para o intervalo entre 0 e 1, que pode ser interpretada como uma probabilidade. A fórmula matemática da função logística é:\n","\n","\\[ P(Y=1|X) = \\frac{1}{1 + e^{-\\beta X}} \\]\n","\n","Onde:\n","- \\( P(Y=1|X) \\) é a probabilidade de que a variável dependente (\\( Y \\)) seja igual a 1 dado o vetor de características (\\( X \\)).\n","- \\( \\beta \\) é um vetor de coeficientes.\n","- \\( e \\) é a base do logaritmo natural (aproximadamente 2.71828).\n","\n","Portanto, a regressão logística é uma técnica que estima os coeficientes (\\( \\beta \\)) com base nos dados de treinamento, permitindo assim a previsão de probabilidades e classificações em problemas de classificação binária."],"metadata":{"id":"nfawX95GZ1fT"}},{"cell_type":"markdown","source":["Se você importou um modelo ou algoritmo em Python e gostaria de conhecer os parâmetros disponíveis ou os hiperparâmetros que podem ser ajustados, há algumas maneiras de fazer isso:\n","\n","1. **Documentação Oficial:** A documentação oficial da biblioteca ou pacote geralmente fornece informações detalhadas sobre os parâmetros e hiperparâmetros disponíveis para cada algoritmo. Isso pode incluir descrições, valores padrão e informações sobre como eles afetam o comportamento do modelo.\n","\n","2. **Função `help()`:** No Python, você pode usar a função embutida `help()` para obter informações sobre um módulo, classe ou função. Basta passar o nome do objeto como argumento para a função. Por exemplo:\n","   \n","   ```python\n","   import sklearn\n","   from sklearn.linear_model import LogisticRegression\n","\n","   # Para obter informações sobre a classe LogisticRegression\n","   help(LogisticRegression)\n","   ```\n","\n","3. **Autocompletar no IDE ou Jupyter Notebook:** Em ambientes como Jupyter Notebook ou IDEs como o Visual Studio Code, você pode digitar o nome do objeto seguido de um ponto e usar o recurso de autocompletar (pressão de tabulação) para ver os métodos e atributos disponíveis.\n","\n","4. **Dir() e Atributos:** A função `dir()` em Python retorna uma lista de nomes de atributos de um objeto. Isso também pode ser útil para explorar os métodos e atributos disponíveis. Por exemplo:\n","\n","   ```python\n","   import sklearn\n","   from sklearn.linear_model import LogisticRegression\n","\n","   # Lista de atributos da classe LogisticRegression\n","   print(dir(LogisticRegression))\n","   ```\n","\n","5. **Explorar a Instância:** Se você criar uma instância do modelo, poderá explorar os atributos e métodos específicos dessa instância para obter informações sobre os parâmetros. Por exemplo:\n","\n","   ```python\n","   import sklearn\n","   from sklearn.linear_model import LogisticRegression\n","\n","   # Criar uma instância do modelo\n","   model = LogisticRegression()\n","\n","   # Listar atributos da instância\n","   print(dir(model))\n","   ```\n","\n","Essas abordagens ajudarão você a descobrir quais parâmetros e hiperparâmetros estão disponíveis para ajuste em um modelo ou algoritmo específico. Certifique-se de consultar a documentação oficial para obter informações completas e detalhadas sobre como ajustar esses parâmetros de acordo com suas necessidades."],"metadata":{"id":"VLztSMuVdMvx"}},{"cell_type":"code","source":["import sklearn\n","from sklearn.linear_model import LogisticRegression\n","\n","# Para obter informações sobre a classe LogisticRegression\n","help(LogisticRegression)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mwmb4n2pZ3kz","executionInfo":{"status":"ok","timestamp":1691671262441,"user_tz":180,"elapsed":1369,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}},"outputId":"a4b74fa9-1bc8-46b4-b3d2-1fa5cf8c7017"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class LogisticRegression in module sklearn.linear_model._logistic:\n","\n","class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n"," |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n"," |  \n"," |  Logistic Regression (aka logit, MaxEnt) classifier.\n"," |  \n"," |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n"," |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n"," |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n"," |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n"," |  'sag', 'saga' and 'newton-cg' solvers.)\n"," |  \n"," |  This class implements regularized logistic regression using the\n"," |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n"," |  that regularization is applied by default**. It can handle both dense\n"," |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n"," |  floats for optimal performance; any other input format will be converted\n"," |  (and copied).\n"," |  \n"," |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n"," |  with primal formulation, or no regularization. The 'liblinear' solver\n"," |  supports both L1 and L2 regularization, with a dual formulation only for\n"," |  the L2 penalty. The Elastic-Net regularization is only supported by the\n"," |  'saga' solver.\n"," |  \n"," |  Read more in the :ref:`User Guide <logistic_regression>`.\n"," |  \n"," |  Parameters\n"," |  ----------\n"," |  penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n"," |      Specify the norm of the penalty:\n"," |  \n"," |      - `None`: no penalty is added;\n"," |      - `'l2'`: add a L2 penalty term and it is the default choice;\n"," |      - `'l1'`: add a L1 penalty term;\n"," |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n"," |  \n"," |      .. warning::\n"," |         Some penalties may not work with some solvers. See the parameter\n"," |         `solver` below, to know the compatibility between the penalty and\n"," |         solver.\n"," |  \n"," |      .. versionadded:: 0.19\n"," |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n"," |  \n"," |      .. deprecated:: 1.2\n"," |         The 'none' option was deprecated in version 1.2, and will be removed\n"," |         in 1.4. Use `None` instead.\n"," |  \n"," |  dual : bool, default=False\n"," |      Dual or primal formulation. Dual formulation is only implemented for\n"," |      l2 penalty with liblinear solver. Prefer dual=False when\n"," |      n_samples > n_features.\n"," |  \n"," |  tol : float, default=1e-4\n"," |      Tolerance for stopping criteria.\n"," |  \n"," |  C : float, default=1.0\n"," |      Inverse of regularization strength; must be a positive float.\n"," |      Like in support vector machines, smaller values specify stronger\n"," |      regularization.\n"," |  \n"," |  fit_intercept : bool, default=True\n"," |      Specifies if a constant (a.k.a. bias or intercept) should be\n"," |      added to the decision function.\n"," |  \n"," |  intercept_scaling : float, default=1\n"," |      Useful only when the solver 'liblinear' is used\n"," |      and self.fit_intercept is set to True. In this case, x becomes\n"," |      [x, self.intercept_scaling],\n"," |      i.e. a \"synthetic\" feature with constant value equal to\n"," |      intercept_scaling is appended to the instance vector.\n"," |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n"," |  \n"," |      Note! the synthetic feature weight is subject to l1/l2 regularization\n"," |      as all other features.\n"," |      To lessen the effect of regularization on synthetic feature weight\n"," |      (and therefore on the intercept) intercept_scaling has to be increased.\n"," |  \n"," |  class_weight : dict or 'balanced', default=None\n"," |      Weights associated with classes in the form ``{class_label: weight}``.\n"," |      If not given, all classes are supposed to have weight one.\n"," |  \n"," |      The \"balanced\" mode uses the values of y to automatically adjust\n"," |      weights inversely proportional to class frequencies in the input data\n"," |      as ``n_samples / (n_classes * np.bincount(y))``.\n"," |  \n"," |      Note that these weights will be multiplied with sample_weight (passed\n"," |      through the fit method) if sample_weight is specified.\n"," |  \n"," |      .. versionadded:: 0.17\n"," |         *class_weight='balanced'*\n"," |  \n"," |  random_state : int, RandomState instance, default=None\n"," |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n"," |      data. See :term:`Glossary <random_state>` for details.\n"," |  \n"," |  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n"," |  \n"," |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n"," |      To choose a solver, you might want to consider the following aspects:\n"," |  \n"," |          - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n"," |            and 'saga' are faster for large ones;\n"," |          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n"," |            'lbfgs' handle multinomial loss;\n"," |          - 'liblinear' is limited to one-versus-rest schemes.\n"," |          - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n"," |            especially with one-hot encoded categorical features with rare\n"," |            categories. Note that it is limited to binary classification and the\n"," |            one-versus-rest reduction for multiclass classification. Be aware that\n"," |            the memory usage of this solver has a quadratic dependency on\n"," |            `n_features` because it explicitly computes the Hessian matrix.\n"," |  \n"," |      .. warning::\n"," |         The choice of the algorithm depends on the penalty chosen.\n"," |         Supported penalties by solver:\n"," |  \n"," |         - 'lbfgs'           -   ['l2', None]\n"," |         - 'liblinear'       -   ['l1', 'l2']\n"," |         - 'newton-cg'       -   ['l2', None]\n"," |         - 'newton-cholesky' -   ['l2', None]\n"," |         - 'sag'             -   ['l2', None]\n"," |         - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n"," |  \n"," |      .. note::\n"," |         'sag' and 'saga' fast convergence is only guaranteed on features\n"," |         with approximately the same scale. You can preprocess the data with\n"," |         a scaler from :mod:`sklearn.preprocessing`.\n"," |  \n"," |      .. seealso::\n"," |         Refer to the User Guide for more information regarding\n"," |         :class:`LogisticRegression` and more specifically the\n"," |         :ref:`Table <Logistic_regression>`\n"," |         summarizing solver/penalty supports.\n"," |  \n"," |      .. versionadded:: 0.17\n"," |         Stochastic Average Gradient descent solver.\n"," |      .. versionadded:: 0.19\n"," |         SAGA solver.\n"," |      .. versionchanged:: 0.22\n"," |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n"," |      .. versionadded:: 1.2\n"," |         newton-cholesky solver.\n"," |  \n"," |  max_iter : int, default=100\n"," |      Maximum number of iterations taken for the solvers to converge.\n"," |  \n"," |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n"," |      If the option chosen is 'ovr', then a binary problem is fit for each\n"," |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n"," |      across the entire probability distribution, *even when the data is\n"," |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n"," |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n"," |      and otherwise selects 'multinomial'.\n"," |  \n"," |      .. versionadded:: 0.18\n"," |         Stochastic Average Gradient descent solver for 'multinomial' case.\n"," |      .. versionchanged:: 0.22\n"," |          Default changed from 'ovr' to 'auto' in 0.22.\n"," |  \n"," |  verbose : int, default=0\n"," |      For the liblinear and lbfgs solvers set verbose to any positive\n"," |      number for verbosity.\n"," |  \n"," |  warm_start : bool, default=False\n"," |      When set to True, reuse the solution of the previous call to fit as\n"," |      initialization, otherwise, just erase the previous solution.\n"," |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n"," |  \n"," |      .. versionadded:: 0.17\n"," |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n"," |  \n"," |  n_jobs : int, default=None\n"," |      Number of CPU cores used when parallelizing over classes if\n"," |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n"," |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n"," |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n"," |      context. ``-1`` means using all processors.\n"," |      See :term:`Glossary <n_jobs>` for more details.\n"," |  \n"," |  l1_ratio : float, default=None\n"," |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n"," |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n"," |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n"," |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n"," |      combination of L1 and L2.\n"," |  \n"," |  Attributes\n"," |  ----------\n"," |  \n"," |  classes_ : ndarray of shape (n_classes, )\n"," |      A list of class labels known to the classifier.\n"," |  \n"," |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n"," |      Coefficient of the features in the decision function.\n"," |  \n"," |      `coef_` is of shape (1, n_features) when the given problem is binary.\n"," |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n"," |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n"," |  \n"," |  intercept_ : ndarray of shape (1,) or (n_classes,)\n"," |      Intercept (a.k.a. bias) added to the decision function.\n"," |  \n"," |      If `fit_intercept` is set to False, the intercept is set to zero.\n"," |      `intercept_` is of shape (1,) when the given problem is binary.\n"," |      In particular, when `multi_class='multinomial'`, `intercept_`\n"," |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n"," |      outcome 0 (False).\n"," |  \n"," |  n_features_in_ : int\n"," |      Number of features seen during :term:`fit`.\n"," |  \n"," |      .. versionadded:: 0.24\n"," |  \n"," |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n"," |      Names of features seen during :term:`fit`. Defined only when `X`\n"," |      has feature names that are all strings.\n"," |  \n"," |      .. versionadded:: 1.0\n"," |  \n"," |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n"," |      Actual number of iterations for all classes. If binary or multinomial,\n"," |      it returns only 1 element. For liblinear solver, only the maximum\n"," |      number of iteration across all classes is given.\n"," |  \n"," |      .. versionchanged:: 0.20\n"," |  \n"," |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n"," |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n"," |  \n"," |  See Also\n"," |  --------\n"," |  SGDClassifier : Incrementally trained logistic regression (when given\n"," |      the parameter ``loss=\"log\"``).\n"," |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n"," |  \n"," |  Notes\n"," |  -----\n"," |  The underlying C implementation uses a random number generator to\n"," |  select features when fitting the model. It is thus not uncommon,\n"," |  to have slightly different results for the same input data. If\n"," |  that happens, try with a smaller tol parameter.\n"," |  \n"," |  Predict output may not match that of standalone liblinear in certain\n"," |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n"," |  in the narrative documentation.\n"," |  \n"," |  References\n"," |  ----------\n"," |  \n"," |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n"," |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n"," |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n"," |  \n"," |  LIBLINEAR -- A Library for Large Linear Classification\n"," |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n"," |  \n"," |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n"," |      Minimizing Finite Sums with the Stochastic Average Gradient\n"," |      https://hal.inria.fr/hal-00860051/document\n"," |  \n"," |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n"," |          :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n"," |          for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n"," |  \n"," |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n"," |      methods for logistic regression and maximum entropy models.\n"," |      Machine Learning 85(1-2):41-75.\n"," |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n"," |  \n"," |  Examples\n"," |  --------\n"," |  >>> from sklearn.datasets import load_iris\n"," |  >>> from sklearn.linear_model import LogisticRegression\n"," |  >>> X, y = load_iris(return_X_y=True)\n"," |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n"," |  >>> clf.predict(X[:2, :])\n"," |  array([0, 0])\n"," |  >>> clf.predict_proba(X[:2, :])\n"," |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n"," |         [9.7...e-01, 2.8...e-02, ...e-08]])\n"," |  >>> clf.score(X, y)\n"," |  0.97...\n"," |  \n"," |  Method resolution order:\n"," |      LogisticRegression\n"," |      sklearn.linear_model._base.LinearClassifierMixin\n"," |      sklearn.base.ClassifierMixin\n"," |      sklearn.linear_model._base.SparseCoefMixin\n"," |      sklearn.base.BaseEstimator\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  fit(self, X, y, sample_weight=None)\n"," |      Fit the model according to the given training data.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          Training vector, where `n_samples` is the number of samples and\n"," |          `n_features` is the number of features.\n"," |      \n"," |      y : array-like of shape (n_samples,)\n"," |          Target vector relative to X.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,) default=None\n"," |          Array of weights that are assigned to individual samples.\n"," |          If not provided, then each sample is given unit weight.\n"," |      \n"," |          .. versionadded:: 0.17\n"," |             *sample_weight* support to LogisticRegression.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self\n"," |          Fitted estimator.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      The SAGA solver supports both float64 and float32 bit arrays.\n"," |  \n"," |  predict_log_proba(self, X)\n"," |      Predict logarithm of probability estimates.\n"," |      \n"," |      The returned estimates for all classes are ordered by the\n"," |      label of classes.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Vector to be scored, where `n_samples` is the number of samples and\n"," |          `n_features` is the number of features.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      T : array-like of shape (n_samples, n_classes)\n"," |          Returns the log-probability of the sample for each class in the\n"," |          model, where classes are ordered as they are in ``self.classes_``.\n"," |  \n"," |  predict_proba(self, X)\n"," |      Probability estimates.\n"," |      \n"," |      The returned estimates for all classes are ordered by the\n"," |      label of classes.\n"," |      \n"," |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n"," |      the softmax function is used to find the predicted probability of\n"," |      each class.\n"," |      Else use a one-vs-rest approach, i.e calculate the probability\n"," |      of each class assuming it to be positive using the logistic function.\n"," |      and normalize these values across all the classes.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Vector to be scored, where `n_samples` is the number of samples and\n"," |          `n_features` is the number of features.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      T : array-like of shape (n_samples, n_classes)\n"," |          Returns the probability of the sample for each class in the model,\n"," |          where classes are ordered as they are in ``self.classes_``.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n"," |  \n"," |  decision_function(self, X)\n"," |      Predict confidence scores for samples.\n"," |      \n"," |      The confidence score for a sample is proportional to the signed\n"," |      distance of that sample to the hyperplane.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The data matrix for which we want to get the confidence scores.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n"," |          Confidence scores per `(n_samples, n_classes)` combination. In the\n"," |          binary case, confidence score for `self.classes_[1]` where >0 means\n"," |          this class would be predicted.\n"," |  \n"," |  predict(self, X)\n"," |      Predict class labels for samples in X.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The data matrix for which we want to get the predictions.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      y_pred : ndarray of shape (n_samples,)\n"," |          Vector containing the class labels for each sample.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the mean accuracy on the given test data and labels.\n"," |      \n"," |      In multi-label classification, this is the subset accuracy\n"," |      which is a harsh metric since you require for each sample that\n"," |      each label set be correctly predicted.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True labels for `X`.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n"," |  \n"," |  densify(self)\n"," |      Convert coefficient matrix to dense array format.\n"," |      \n"," |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n"," |      default format of ``coef_`` and is required for fitting, so calling\n"," |      this method is only required on models that have previously been\n"," |      sparsified; otherwise, it is a no-op.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self\n"," |          Fitted estimator.\n"," |  \n"," |  sparsify(self)\n"," |      Convert coefficient matrix to sparse format.\n"," |      \n"," |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n"," |      L1-regularized models can be much more memory- and storage-efficient\n"," |      than the usual numpy.ndarray representation.\n"," |      \n"," |      The ``intercept_`` member is not converted.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self\n"," |          Fitted estimator.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n"," |      this may actually *increase* memory usage, so use this method with\n"," |      care. A rule of thumb is that the number of zero elements, which can\n"," |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n"," |      to provide significant benefits.\n"," |      \n"," |      After calling this method, further fitting with the partial_fit\n"," |      method (if any) will not work until you call densify.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |  \n"," |  __getstate__(self)\n"," |  \n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |  \n"," |  __setstate__(self, state)\n"," |  \n"," |  get_params(self, deep=True)\n"," |      Get parameters for this estimator.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      deep : bool, default=True\n"," |          If True, will return the parameters for this estimator and\n"," |          contained subobjects that are estimators.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      params : dict\n"," |          Parameter names mapped to their values.\n"," |  \n"," |  set_params(self, **params)\n"," |      Set the parameters of this estimator.\n"," |      \n"," |      The method works on simple estimators as well as on nested objects\n"," |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n"," |      parameters of the form ``<component>__<parameter>`` so that it's\n"," |      possible to update each component of a nested object.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      **params : dict\n"," |          Estimator parameters.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : estimator instance\n"," |          Estimator instance.\n","\n"]}]},{"cell_type":"code","source":["import sklearn\n","from sklearn.linear_model import LogisticRegression\n","\n","# Lista de atributos da classe LogisticRegression\n","print(dir(LogisticRegression))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzP51Rl8eGcJ","executionInfo":{"status":"ok","timestamp":1691671385520,"user_tz":180,"elapsed":315,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}},"outputId":"befc6080-9114-48b0-82af-8337e72d5f4d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_check_feature_names', '_check_n_features', '_estimator_type', '_get_param_names', '_get_tags', '_more_tags', '_parameter_constraints', '_predict_proba_lr', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_validate_data', '_validate_params', 'decision_function', 'densify', 'fit', 'get_params', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params', 'sparsify']\n"]}]},{"cell_type":"markdown","source":["\n","Tanto a classificação quanto a regressão são técnicas de aprendizado de máquina que lidam com diferentes tipos de problemas e tipos de saída desejada:\n","\n","1. **Classificação**: Nessa técnica, o objetivo é atribuir uma instância de entrada a uma categoria ou classe predefinida. Essas classes podem ser binárias (duas categorias possíveis, como sim/não) ou multinomiais (várias categorias possíveis, como vermelho/verde/azul). A classificação é utilizada quando a saída desejada é discreta e categórica. Isso não necessariamente se limita a prever o presente, pois você pode estar classificando dados históricos ou eventos passados em categorias.\n","\n","2. **Regressão**: Nessa técnica, o objetivo é prever um valor numérico contínuo com base nas entradas. Em vez de categorias, você está prevendo um número real. A regressão é comumente usada para modelar relações entre variáveis e prever tendências futuras com base em dados históricos. No entanto, a regressão não está restrita a prever o futuro; ela se concentra em encontrar padrões e relações entre variáveis, independentemente de serem valores atuais ou futuros.\n","\n","Em resumo, você está certo em considerar que a regressão pode ser usada para prever tendências futuras, mas também pode ser usada para modelar relações entre variáveis presentes. A classificação, por sua vez, trata de categorizar dados em classes específicas, independentemente de se referirem a situações presentes ou passadas. A escolha entre classificação e regressão depende mais da natureza dos dados e dos objetivos específicos do projeto do que uma divisão estrita entre prever o presente ou o futuro."],"metadata":{"id":"FtYQQWk_G9Sf"}},{"cell_type":"code","source":[],"metadata":{"id":"C7YRy7OJG_M-"},"execution_count":null,"outputs":[]}]}