{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMpQ/QqfP4wBPoO1XrSNOe6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","1. `pyspark.sql` é usado para manipulação e otimização: O `pyspark.sql` fornece uma API mais amigável para manipular e consultar dados usando DataFrames, que são estruturas de dados tabulares e distribuídas. O Spark SQL também é responsável por otimizar as operações no DataFrame para executá-las de forma eficiente em um ambiente distribuído. Isso significa que, quando você executa operações em um DataFrame usando `pyspark.sql`, o Spark SQL otimiza as operações e as traduz em um plano de execução mais eficiente, que é executado no RDD.\n","\n","2. RDD executa as operações reais: O RDD (Resilient Distributed Datasets) é a estrutura de dados principal do Spark e representa uma coleção distribuída e imutável de objetos. Os RDDs são a base de todo o processamento do Spark e são responsáveis por executar as operações reais. Quando você executa operações em um DataFrame usando o `pyspark.sql`, o Spark SQL traduz essas operações em transformações e ações no RDD subjacente, que é onde as operações são efetivamente executadas.\n","\n","3. Integração entre `pyspark.sql` e RDD: O `pyspark.sql` e o RDD estão interconectados no Spark. O `pyspark.sql` fornece abstrações de nível superior como DataFrames e permite que você consulte dados usando a sintaxe SQL ou DSL (Domain-Specific Language) do DataFrame. Internamente, o Spark SQL otimiza essas operações e as transforma em operações no RDD. Portanto, o `pyspark.sql` usa o RDD como sua base para a execução.\n","\n","4. Escolha entre `pyspark.sql` e RDD: Em geral, para operações comuns de manipulação e consultas de dados, é recomendado usar o `pyspark.sql` com DataFrames, pois eles oferecem uma interface mais fácil de usar e otimizações de consulta. Os RDDs são mais adequados para situações mais complexas ou personalizadas, onde você precisa de maior flexibilidade ou controle sobre o processamento. No entanto, a maioria dos casos de uso pode ser tratada de forma eficiente usando DataFrames com `pyspark.sql`.\n","\n","Portanto, em resumo, o `pyspark.sql` é usado para manipulação e consulta de dados usando DataFrames, que são estruturas de dados tabulares. O Spark SQL otimiza as operações no DataFrame e as executa no RDD subjacente, que é a estrutura de dados principal do Spark usada para executar as operações reais."],"metadata":{"id":"nt5x8K1rfx7K"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLs0GXF8ccXS","executionInfo":{"status":"ok","timestamp":1691285141092,"user_tz":180,"elapsed":34079,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}},"outputId":"3ae12c5f-783d-4e94-ae63-ee55ffe17d19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285397 sha256=6658080b7ec3e81a5c4cd68fda25143926a315cc812fe7d9d9e5632f13232939\n","  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.1\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["from pyspark import SparkContext"],"metadata":{"id":"cVpa3z4Ycl7Z","executionInfo":{"status":"ok","timestamp":1691285165243,"user_tz":180,"elapsed":280,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["sc = SparkContext()"],"metadata":{"id":"s8QTmPE_cvDw","executionInfo":{"status":"ok","timestamp":1691285182923,"user_tz":180,"elapsed":5739,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["data = [1,2,3,4,5]"],"metadata":{"id":"X34N9BMcc3n8","executionInfo":{"status":"ok","timestamp":1691285196223,"user_tz":180,"elapsed":3,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["rdd = sc.parallelize(data)"],"metadata":{"id":"cGNgJ3BsdAoF","executionInfo":{"status":"ok","timestamp":1691285261118,"user_tz":180,"elapsed":723,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["rdd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUmVP63XdQWp","executionInfo":{"status":"ok","timestamp":1691285266293,"user_tz":180,"elapsed":5,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}},"outputId":"0cc8a5c4-e982-4e29-e2d8-5635ddc6fbef"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# operações de ação\n","\n","# exibindo dados, como os dados vai estar em nó quando exibir ele uni\n","rdd.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQEXs6GxdRru","executionInfo":{"status":"ok","timestamp":1691285336576,"user_tz":180,"elapsed":832,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}},"outputId":"777a0ac1-be3b-4ea6-ce67-85c378fb7b63"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3, 4, 5]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# O método reduce é uma das ações disponíveis para RDDs (Resilient Distributed Datasets) no Spark.\n","# Ele é usado para realizar uma redução de todos os elementos do RDD para um único valor.\n","# A operação de redução é aplicada de forma iterativa\n","# a todos os elementos do RDD, reduzindo gradualmente o conjunto de dados a um único valor resultante.\n","rdd.reduce(lambda x, y: x + y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1xJIJBFldiqz","executionInfo":{"status":"ok","timestamp":1691285458674,"user_tz":180,"elapsed":1209,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}},"outputId":"add7e96a-efb5-4310-9e05-7c0707899b68"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["rdd.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hqv-ZAtKeAW-","executionInfo":{"status":"ok","timestamp":1691285962760,"user_tz":180,"elapsed":448,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}},"outputId":"07b5d9a4-5adb-4500-e29a-157187f4ef8d"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# operações de transformação\n","\n","# tem que criar uma nova variavel porque ele vai salvar os dados em um novo objeto ou nova rdd então cria uma nova variavel representando a nova rdd\n","rdd2 = rdd.map(lambda x: x +10)"],"metadata":{"id":"9Uesvw4lf7qF","executionInfo":{"status":"ok","timestamp":1691286395017,"user_tz":180,"elapsed":295,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["rdd.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8U0P2apQgrq9","executionInfo":{"status":"ok","timestamp":1691286240399,"user_tz":180,"elapsed":3,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}},"outputId":"ac3141b1-5ff3-4c01-ea79-d6d9db7bc0d9"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3, 4, 5]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["rdd2.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ste-ZnV6g_lF","executionInfo":{"status":"ok","timestamp":1691286396659,"user_tz":180,"elapsed":431,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}},"outputId":"822157ad-a93b-4cac-de4c-018b799727b3"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[11, 12, 13, 14, 15]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# tem que criar uma nova variavel porque ele vai salvar os dados em um novo objeto ou nova rdd então cria uma nova variavel representando a nova rdd\n","rdd3 = rdd.filter(lambda x: x%2 ==0)"],"metadata":{"id":"XQfBa0_ihkY5","executionInfo":{"status":"ok","timestamp":1691286540595,"user_tz":180,"elapsed":675,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["rdd3.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gx2_rm1RiJeP","executionInfo":{"status":"ok","timestamp":1691286560865,"user_tz":180,"elapsed":20,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}},"outputId":"f24f189d-3f6f-4380-9901-50f84b90a588"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 4]"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# Ao usar o Spark, é uma boa prática parar o contexto do Spark antes de finalizar\n","# o programa ou encerrar uma sessão interativa, como uma sessão do Jupyter Notebook.\n","# Isso evita conflitos e problemas relacionados ao uso de recursos do cluster Spark ou do ambiente local.\n","sc.stop()"],"metadata":{"id":"cFR5jwPSiNCd","executionInfo":{"status":"ok","timestamp":1691286780927,"user_tz":180,"elapsed":404,"user":{"displayName":"Daniel Oliveira","userId":"17424629218949984830"}}},"execution_count":23,"outputs":[]}]}